package app

import (
	"bytes"
	"encoding/json"
	"fmt"
	"log"
	"slices"
	"strings"
)

// ============================================================================
// SSE Usage è§£æå™¨ (é‡æ„ç‰ˆ - éµå¾ªSRP)
// ============================================================================

// sseUsageParser SSEæµå¼å“åº”çš„usageæ•°æ®è§£æå™¨
// è®¾è®¡åŸåˆ™ï¼ˆSRPï¼‰ï¼šä»…è´Ÿè´£ä»SSEäº‹ä»¶æµä¸­æå–tokenç»Ÿè®¡ä¿¡æ¯ï¼Œä¸è´Ÿè´£I/O
// é‡‡ç”¨å¢é‡è§£æé¿å…é‡å¤æ‰«æï¼ˆO(nÂ²) â†’ O(n)ï¼‰
type usageAccumulator struct {
	InputTokens              int
	OutputTokens             int
	CacheReadInputTokens     int
	CacheCreationInputTokens int
}

type sseUsageParser struct {
	usageAccumulator

	// å†…éƒ¨çŠ¶æ€ï¼ˆå¢é‡è§£æï¼‰
	buffer      bytes.Buffer // æœªå®Œæˆçš„æ•°æ®ç¼“å†²åŒº
	bufferSize  int          // å½“å‰ç¼“å†²åŒºå¤§å°
	eventType   string       // å½“å‰æ­£åœ¨è§£æçš„äº‹ä»¶ç±»å‹ï¼ˆè·¨Feedä¿å­˜ï¼‰
	dataLines   []string     // å½“å‰äº‹ä»¶çš„dataè¡Œï¼ˆè·¨Feedä¿å­˜ï¼‰
	oversized   bool         // æ ‡è®°æ˜¯å¦è¶…å‡ºå¤§å°é™åˆ¶ï¼ˆåœæ­¢è§£æä½†ä¸ä¸­æ–­æµä¼ è¾“ï¼‰
	channelType string       // æ¸ é“ç±»å‹(anthropic/openai/codex/gemini),ç”¨äºç²¾ç¡®å¹³å°åˆ¤æ–­

	// âœ… æ–°å¢ï¼šå­˜å‚¨SSEæµä¸­æ£€æµ‹åˆ°çš„erroräº‹ä»¶ï¼ˆç”¨äº1308ç­‰é”™è¯¯çš„å»¶è¿Ÿå¤„ç†ï¼‰
	lastError []byte // æœ€åä¸€ä¸ªerroräº‹ä»¶çš„å®Œæ•´JSONï¼ˆdataå­—æ®µå†…å®¹ï¼‰
}

type jsonUsageParser struct {
	usageAccumulator
	buffer      bytes.Buffer
	truncated   bool
	channelType string // æ¸ é“ç±»å‹(anthropic/openai/codex/gemini),ç”¨äºç²¾ç¡®å¹³å°åˆ¤æ–­
}

type usageParser interface {
	Feed([]byte) error
	GetUsage() (inputTokens, outputTokens, cacheRead, cacheCreation int)
	GetLastError() []byte // âœ… æ–°å¢ï¼šè¿”å›SSEæµä¸­æ£€æµ‹åˆ°çš„æœ€åä¸€ä¸ªerroräº‹ä»¶ï¼ˆç”¨äº1308ç­‰é”™è¯¯çš„å»¶è¿Ÿå¤„ç†ï¼‰
	GetReceivedData() []byte // âœ… æ–°å¢ï¼šè¿”å›æ¥æ”¶åˆ°çš„åŸå§‹æ•°æ®ï¼ˆç”¨äºè¯Šæ–­æµä¸å®Œæ•´é—®é¢˜ï¼‰
}

const (
	// maxSSEEventSize SSEäº‹ä»¶æœ€å¤§å°ºå¯¸ï¼ˆé˜²æ­¢å†…å­˜è€—å°½æ”»å‡»ï¼‰
	maxSSEEventSize = 1 << 20 // 1MB

	// maxUsageBodySize ç”¨äºæ™®é€šJSONå“åº” usage æå–æ—¶çš„æœ€å¤§ç¼“å­˜ï¼ˆé˜²æ­¢å†…å­˜è¿‡å¤§ï¼‰
	maxUsageBodySize = 1 << 20 // 1MB
)

// newSSEUsageParser åˆ›å»ºSSE usageè§£æå™¨
// channelType: æ¸ é“ç±»å‹(anthropic/openai/codex/gemini),ç”¨äºç²¾ç¡®è¯†åˆ«å¹³å°usageæ ¼å¼
func newSSEUsageParser(channelType string) *sseUsageParser {
	return &sseUsageParser{channelType: channelType}
}

// newJSONUsageParser åˆ›å»ºJSONå“åº”çš„usageè§£æå™¨
// channelType: æ¸ é“ç±»å‹(anthropic/openai/codex/gemini),ç”¨äºç²¾ç¡®è¯†åˆ«å¹³å°usageæ ¼å¼
func newJSONUsageParser(channelType string) *jsonUsageParser {
	return &jsonUsageParser{channelType: channelType}
}

// Feed å–‚å…¥æ•°æ®è¿›è¡Œè§£æï¼ˆä¾›streamCopySSEè°ƒç”¨ï¼‰
// é‡‡ç”¨å¢é‡è§£æï¼Œé¿å…é‡å¤æ‰«æå·²å¤„ç†æ•°æ®
func (p *sseUsageParser) Feed(data []byte) error {
	// å¦‚æœå·²æ ‡è®°ä¸ºè¶…é™,ä¸å†è§£æusageä½†ç»§ç»­ä¼ è¾“æµ
	if p.oversized {
		return nil
	}

	// é˜²å¾¡æ€§æ£€æŸ¥:é™åˆ¶ç¼“å†²åŒºå¤§å°
	if p.bufferSize+len(data) > maxSSEEventSize {
		log.Printf("WARN: SSE usage buffer exceeds max size (%d bytes), stopping usage extraction for this request", maxSSEEventSize)
		p.oversized = true
		return nil // ä¸è¿”å›é”™è¯¯,è®©æµä¼ è¾“ç»§ç»­
	}

	// ğŸ” è¯Šæ–­è¡¥ä¸: è®°å½•å¼‚å¸¸å°çš„é¦–å—æ•°æ®(ç”¨äºå®šä½21å­—èŠ‚é—®é¢˜)
	// æ­£å¸¸SSEäº‹ä»¶è‡³å°‘40-50å­—èŠ‚,å¦‚æœé¦–å—<64å­—èŠ‚å¯èƒ½æ˜¯ä¸Šæ¸¸å¼‚å¸¸
	if p.bufferSize == 0 && len(data) <= 64 {
		log.Printf("ğŸ” [SSEå¼‚å¸¸é¦–å—] æ¸ é“=%s å¤§å°=%d å†…å®¹=%q", 
			p.channelType, len(data), data)
	}

	p.buffer.Write(data)
	p.bufferSize += len(data)
	return p.parseBuffer()
}

// parseBuffer è§£æç¼“å†²åŒºä¸­çš„SSEäº‹ä»¶ï¼ˆå¢é‡è§£æï¼‰
func (p *sseUsageParser) parseBuffer() error {
	bufData := p.buffer.Bytes()
	offset := 0

	for {
		// æŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ¢è¡Œç¬¦
		lineEnd := bytes.IndexByte(bufData[offset:], '\n')
		if lineEnd == -1 {
			// æ²¡æœ‰å®Œæ•´çš„è¡Œï¼Œä¿ç•™å‰©ä½™æ•°æ®
			break
		}

		// æå–å½“å‰è¡Œï¼ˆå»é™¤\r\nï¼‰
		lineEnd += offset
		line := string(bytes.TrimRight(bufData[offset:lineEnd], "\r"))
		offset = lineEnd + 1

		// SSEäº‹ä»¶æ ¼å¼ï¼š
		// event: message_start
		// data: {...}
		// (ç©ºè¡Œè¡¨ç¤ºäº‹ä»¶ç»“æŸ)

		if after, ok := strings.CutPrefix(line, "event:"); ok {
			p.eventType = strings.TrimSpace(after)
		} else if after0, ok0 := strings.CutPrefix(line, "data:"); ok0 {
			dataLine := strings.TrimSpace(after0)
			p.dataLines = append(p.dataLines, dataLine)
		} else if line == "" && len(p.dataLines) > 0 {
			// äº‹ä»¶ç»“æŸï¼Œè§£ææ•°æ®
			if err := p.parseEvent(p.eventType, strings.Join(p.dataLines, "")); err != nil {
				// è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ï¼ˆå®¹é”™è®¾è®¡ï¼‰
				log.Printf("WARN: SSE event parse failed (type=%s): %v", p.eventType, err)
			}
			p.eventType = ""
			p.dataLines = nil
		}
	}

	// ä¿ç•™æœªå¤„ç†çš„æ•°æ®ï¼ˆä»offsetå¼€å§‹ï¼‰
	if offset > 0 {
		remaining := bufData[offset:]
		p.buffer.Reset()
		p.buffer.Write(remaining)
		p.bufferSize = len(remaining)
	}

	return nil
}

// parseEvent è§£æå•ä¸ªSSEäº‹ä»¶
func (p *sseUsageParser) parseEvent(eventType, data string) error {
	// âœ… äº‹ä»¶ç±»å‹è¿‡æ»¤ä¼˜åŒ–ï¼ˆ2025-12-07ï¼‰
	// é—®é¢˜ï¼šanyrouterç­‰èšåˆæœåŠ¡ä½¿ç”¨éæ ‡å‡†äº‹ä»¶ç±»å‹ï¼ˆå¦‚"."ï¼‰ï¼Œå¯¼è‡´usageä¸¢å¤±
	// æ–¹æ¡ˆï¼šæ”¹ä¸ºé»‘åå•æ¨¡å¼ - åªè¿‡æ»¤å·²çŸ¥æ— ç”¨äº‹ä»¶ï¼Œå…¶ä»–éƒ½å°è¯•è§£æ

	// âš ï¸ ç‰¹æ®Šå¤„ç†ï¼šerroräº‹ä»¶ï¼ˆè®°å½•æ—¥å¿— + å­˜å‚¨é”™è¯¯ä½“ç”¨äºåç»­å†·å´å¤„ç†ï¼‰
	if eventType == "error" {
		log.Printf("âš ï¸  [SSEé”™è¯¯äº‹ä»¶] ä¸Šæ¸¸è¿”å›erroräº‹ä»¶: %s", data)
		// âœ… æ–°å¢ï¼šå­˜å‚¨é”™è¯¯äº‹ä»¶çš„å®Œæ•´JSONï¼ˆç”¨äºæµç»“æŸåè§¦å‘å†·å´é€»è¾‘ï¼‰
		p.lastError = []byte(data)
		return nil // ä¸è§£æusageï¼Œé¿å…è¯¯åˆ¤
	}

	// å·²çŸ¥æ— ç”¨äº‹ä»¶ï¼ˆä¸åŒ…å«usageï¼‰
	ignoredEvents := []string{
		"ping",                // å¿ƒè·³äº‹ä»¶
		"content_block_start", // Claudeå†…å®¹å—å¼€å§‹ï¼ˆæ— usageï¼‰
		"content_block_delta", // Claudeå¢é‡å†…å®¹ï¼ˆæ— usageï¼‰
	}

	if eventType != "" && slices.Contains(ignoredEvents, eventType) {
		return nil // è·³è¿‡å·²çŸ¥æ— ç”¨äº‹ä»¶
	}

	// è§£æJSONæ•°æ®
	var event map[string]any
	if err := json.Unmarshal([]byte(data), &event); err != nil {
		return fmt.Errorf("json unmarshal failed: %w", err)
	}

	usage := extractUsage(event)

	if usage == nil {
		return nil
	}

	p.applyUsage(usage, p.channelType)

	return nil
}

// GetUsage è·å–ç´¯ç§¯çš„usageç»Ÿè®¡
// é‡è¦: è¿”å›çš„inputTokenså·²å½’ä¸€åŒ–ä¸º"å¯è®¡è´¹è¾“å…¥token"
// - OpenAI/Codex: prompt_tokensåŒ…å«cached_tokensï¼Œå·²è‡ªåŠ¨æ‰£é™¤é¿å…åŒè®¡
// - Claude/Gemini: input_tokensæœ¬èº«å°±æ˜¯éç¼“å­˜éƒ¨åˆ†ï¼Œæ— éœ€å¤„ç†
func (p *sseUsageParser) GetUsage() (inputTokens, outputTokens, cacheRead, cacheCreation int) {
	billableInput := p.InputTokens

	// OpenAIè¯­ä¹‰å½’ä¸€åŒ–: prompt_tokensåŒ…å«cached_tokensï¼Œéœ€æ‰£é™¤
	// è®¾è®¡åŸåˆ™: å¹³å°å·®å¼‚åœ¨è§£æå±‚å¤„ç†ï¼Œè®¡è´¹å±‚æ— éœ€å…³å¿ƒ
	if (p.channelType == "openai" || p.channelType == "codex") && p.CacheReadInputTokens > 0 {
		billableInput = p.InputTokens - p.CacheReadInputTokens
		if billableInput < 0 {
			log.Printf("WARN: %s model has cacheReadTokens(%d) > inputTokens(%d), clamped to 0",
				p.channelType, p.CacheReadInputTokens, p.InputTokens)
			billableInput = 0
		}
	}

	return billableInput, p.OutputTokens, p.CacheReadInputTokens, p.CacheCreationInputTokens
}

// âœ… GetLastError è¿”å›SSEæµä¸­æ£€æµ‹åˆ°çš„æœ€åä¸€ä¸ªerroräº‹ä»¶
func (p *sseUsageParser) GetLastError() []byte {
	return p.lastError
}

func (p *sseUsageParser) GetReceivedData() []byte {
	return p.buffer.Bytes()
}

func (p *jsonUsageParser) Feed(data []byte) error {
	if p.truncated {
		return nil
	}
	if p.buffer.Len()+len(data) > maxUsageBodySize {
		p.truncated = true
		log.Printf("WARN: usage body exceeds max size (%d bytes), skip usage extraction", maxUsageBodySize)
		return nil
	}
	_, err := p.buffer.Write(data)
	return err
}

func (p *jsonUsageParser) GetUsage() (inputTokens, outputTokens, cacheRead, cacheCreation int) {
	if p.truncated || p.buffer.Len() == 0 {
		return 0, 0, 0, 0
	}

	data := p.buffer.Bytes()

	// å…¼å®¹ text/plain SSE å›é€€ï¼šä¸Šæ¸¸å¶å°”ç”¨ text/plain å‘é€ SSE äº‹ä»¶
	if bytes.Contains(data, []byte("event:")) {
		sseParser := &sseUsageParser{channelType: p.channelType}
		if err := sseParser.Feed(data); err != nil {
			log.Printf("WARN: usage sse-like parse failed: %v", err)
		} else {
			return sseParser.GetUsage()
		}
	}

	var payload map[string]any
	if err := json.Unmarshal(data, &payload); err != nil {
		log.Printf("WARN: usage json parse failed: %v", err)
		return 0, 0, 0, 0
	}

	p.applyUsage(extractUsage(payload), p.channelType)

	// OpenAIè¯­ä¹‰å½’ä¸€åŒ–: ä¸sseUsageParserä¿æŒä¸€è‡´
	billableInput := p.InputTokens
	if (p.channelType == "openai" || p.channelType == "codex") && p.CacheReadInputTokens > 0 {
		billableInput = p.InputTokens - p.CacheReadInputTokens
		if billableInput < 0 {
			log.Printf("WARN: %s model has cacheReadTokens(%d) > inputTokens(%d), clamped to 0",
				p.channelType, p.CacheReadInputTokens, p.InputTokens)
			billableInput = 0
		}
	}

	return billableInput, p.OutputTokens, p.CacheReadInputTokens, p.CacheCreationInputTokens
}

// âœ… GetLastError è¿”å›nilï¼ˆjsonUsageParserä¸å¤„ç†SSE erroräº‹ä»¶ï¼‰
func (p *jsonUsageParser) GetLastError() []byte {
	return nil // JSONè§£æå™¨ä¸å¤„ç†SSE erroräº‹ä»¶
}

func (p *jsonUsageParser) GetReceivedData() []byte {
	return p.buffer.Bytes()
}

func (u *usageAccumulator) applyUsage(usage map[string]any, channelType string) {
	if usage == nil {
		return
	}

	// å¹³å°åˆ¤æ–­:ä¼˜å…ˆä½¿ç”¨channelType(é…ç½®æ˜ç¡®),fallbackåˆ°å­—æ®µç‰¹å¾æ£€æµ‹
	// è®¾è®¡åŸåˆ™:Trust Configuration > Guess from Data
	switch channelType {
	case "gemini":
		// Geminiå¹³å°:usageMetadataåŒ…è£…æˆ–ç›´æ¥å­—æ®µ
		u.applyGeminiUsage(usage)

	case "openai", "codex":
		// OpenAIå¹³å°:éœ€åŒºåˆ†Chat Completions vs Responses API
		// Chat Completions: prompt_tokens + completion_tokens
		// Responses API: input_tokens + output_tokens
		if hasOpenAIChatUsageFields(usage) {
			u.applyOpenAIChatUsage(usage)
		} else if hasAnthropicUsageFields(usage) {
			// OpenAI Responses APIä½¿ç”¨ç±»ä¼¼Anthropicçš„å­—æ®µ
			u.applyAnthropicOrResponsesUsage(usage)
		} else {
			log.Printf("WARN: OpenAI channel with unknown usage format, keys: %v", getUsageKeys(usage))
		}

	case "anthropic":
		// Anthropicå¹³å°:input_tokens + output_tokens + cacheå­—æ®µ
		u.applyAnthropicOrResponsesUsage(usage)

	default:
		// æœªçŸ¥channelType,fallbackåˆ°å­—æ®µç‰¹å¾æ£€æµ‹(å‘åå…¼å®¹)
		log.Printf("WARN: unknown channel_type '%s', fallback to field detection", channelType)
		switch {
		case hasGeminiUsageFields(usage):
			u.applyGeminiUsage(usage)
		case hasOpenAIChatUsageFields(usage):
			u.applyOpenAIChatUsage(usage)
		case hasAnthropicUsageFields(usage):
			u.applyAnthropicOrResponsesUsage(usage)
		default:
			log.Printf("ERROR: cannot detect usage format for channel_type '%s', keys: %v", channelType, getUsageKeys(usage))
		}
	}
}

// hasGeminiUsageFields æ£€æµ‹æ˜¯å¦ä¸ºGemini usageæ ¼å¼
// ç»„åˆåˆ¤æ–­:usageMetadata(åŒ…è£…) æˆ– promptTokenCount+candidatesTokenCount(ç›´æ¥å­—æ®µ)
func hasGeminiUsageFields(usage map[string]any) bool {
	// æ£€æŸ¥usageMetadataåŒ…è£…æ ¼å¼
	if _, ok := usage["usageMetadata"].(map[string]any); ok {
		return true
	}
	// æ£€æŸ¥ç›´æ¥å­—æ®µæ ¼å¼(è‡³å°‘æœ‰ä¸€ä¸ªGeminiç‰¹æœ‰å­—æ®µ)
	_, hasPromptCount := usage["promptTokenCount"].(float64)
	_, hasCandidatesCount := usage["candidatesTokenCount"].(float64)
	return hasPromptCount || hasCandidatesCount
}

// hasOpenAIChatUsageFields æ£€æµ‹æ˜¯å¦ä¸ºOpenAI Chat Completionsæ ¼å¼
// ç»„åˆåˆ¤æ–­:å¿…é¡»æœ‰prompt_tokenså’Œcompletion_tokens
func hasOpenAIChatUsageFields(usage map[string]any) bool {
	_, hasPromptTokens := usage["prompt_tokens"].(float64)
	_, hasCompletionTokens := usage["completion_tokens"].(float64)
	// OpenAI Chatæ ¼å¼å¿…é¡»åŒæ—¶æœ‰è¿™ä¸¤ä¸ªå­—æ®µ
	return hasPromptTokens && hasCompletionTokens
}

// hasAnthropicUsageFields æ£€æµ‹æ˜¯å¦ä¸ºAnthropic/OpenAI Responsesæ ¼å¼
// ç»„åˆåˆ¤æ–­:è‡³å°‘æœ‰input_tokensæˆ–output_tokensä¹‹ä¸€
func hasAnthropicUsageFields(usage map[string]any) bool {
	_, hasInputTokens := usage["input_tokens"].(float64)
	_, hasOutputTokens := usage["output_tokens"].(float64)
	return hasInputTokens || hasOutputTokens
}

// applyGeminiUsage å¤„ç†Geminiæ ¼å¼çš„usage
func (u *usageAccumulator) applyGeminiUsage(usage map[string]any) {
	if val, ok := usage["promptTokenCount"].(float64); ok {
		u.InputTokens = int(val)
	}

	// è¾“å‡ºtoken = candidatesTokenCount + thoughtsTokenCount
	// Gemini 2.5 Proç­‰æ¨¡å‹çš„æ€è€ƒtokenéœ€è¦è®¡å…¥è¾“å‡º
	var outputTokens int
	if val, ok := usage["candidatesTokenCount"].(float64); ok {
		outputTokens = int(val)
	}
	if val, ok := usage["thoughtsTokenCount"].(float64); ok {
		outputTokens += int(val)
	}

	// å¤‡é€‰æ–¹æ¡ˆï¼šå½“candidatesTokenCountä¸º0æ—¶ï¼Œå°è¯•ä»totalTokenCountæ¨ç®—
	// æŸäº›Geminiæ¨¡å‹çš„æµå¼å“åº”ä¸­candidatesTokenCountå§‹ç»ˆä¸º0
	if outputTokens == 0 {
		if total, ok := usage["totalTokenCount"].(float64); ok {
			if prompt, ok := usage["promptTokenCount"].(float64); ok {
				calculated := int(total) - int(prompt)
				if calculated > 0 {
					outputTokens = calculated
				}
			}
		}
	}

	u.OutputTokens = outputTokens
	// Geminiç›®å‰ä¸æ”¯æŒç¼“å­˜å­—æ®µ
}

// applyOpenAIChatUsage å¤„ç†OpenAI Chat Completions APIæ ¼å¼
func (u *usageAccumulator) applyOpenAIChatUsage(usage map[string]any) {
	if val, ok := usage["prompt_tokens"].(float64); ok {
		u.InputTokens = int(val)
	}
	if val, ok := usage["completion_tokens"].(float64); ok {
		u.OutputTokens = int(val)
	}
	// OpenAI Chat Completionsç¼“å­˜å­—æ®µ: prompt_tokens_details.cached_tokens
	if details, ok := usage["prompt_tokens_details"].(map[string]any); ok {
		if val, ok := details["cached_tokens"].(float64); ok {
			u.CacheReadInputTokens = int(val)
		}
	}
}

// applyAnthropicOrResponsesUsage å¤„ç†Anthropicæˆ–OpenAI Responses APIæ ¼å¼
// é‡è¦ï¼šAnthropic SSEæµä¸­ï¼Œmessage_startåŒ…å«input_tokensï¼Œmessage_deltaåŒ…å«cumulative output_tokens
// æŸäº›ä¸­é—´ä»£ç†ï¼ˆå¦‚anyrouterï¼‰ä¼šåœ¨message_deltaä¸­æ·»åŠ input_tokens:0ï¼Œéœ€è¦é˜²å¾¡æ€§å¤„ç†
func (u *usageAccumulator) applyAnthropicOrResponsesUsage(usage map[string]any) {
	// input_tokens: åªæœ‰ > 0 æ—¶æ‰è¦†ç›–ï¼ˆé˜²æ­¢message_deltaä¸­çš„0è¦†ç›–message_startçš„æ­£ç¡®å€¼ï¼‰
	if val, ok := usage["input_tokens"].(float64); ok && int(val) > 0 {
		u.InputTokens = int(val)
	}
	// output_tokens: ç›´æ¥è¦†ç›–ï¼ˆcumulativeè¯­ä¹‰ï¼Œåç»­å€¼åŒ…å«ä¹‹å‰çš„ç´¯è®¡ï¼‰
	if val, ok := usage["output_tokens"].(float64); ok {
		u.OutputTokens = int(val)
	}

	// Anthropicç¼“å­˜å­—æ®µ
	if val, ok := usage["cache_read_input_tokens"].(float64); ok {
		u.CacheReadInputTokens = int(val)
	}
	if val, ok := usage["cache_creation_input_tokens"].(float64); ok {
		u.CacheCreationInputTokens = int(val)
	}

	// OpenAI Responses APIç¼“å­˜å­—æ®µ: input_tokens_details.cached_tokens
	if details, ok := usage["input_tokens_details"].(map[string]any); ok {
		if val, ok := details["cached_tokens"].(float64); ok {
			u.CacheReadInputTokens = int(val)
		}
	}
}

// getUsageKeys è·å–usage mapçš„æ‰€æœ‰keyç”¨äºæ—¥å¿—
func getUsageKeys(usage map[string]any) []string {
	keys := make([]string, 0, len(usage))
	for k := range usage {
		keys = append(keys, k)
	}
	return keys
}

func extractUsage(payload map[string]any) map[string]any {
	// Claude/OpenAIæ ¼å¼: {"usage": {...}}
	if usage, ok := payload["usage"].(map[string]any); ok {
		return usage
	}
	// Claudeæ¶ˆæ¯æ ¼å¼: {"message": {"usage": {...}}}
	if msg, ok := payload["message"].(map[string]any); ok {
		if usage, ok := msg["usage"].(map[string]any); ok {
			return usage
		}
	}
	// OpenAIéƒ¨åˆ†æ ¼å¼: {"response": {"usage": {...}}}
	if resp, ok := payload["response"].(map[string]any); ok {
		if usage, ok := resp["usage"].(map[string]any); ok {
			return usage
		}
	}
	// Geminiæ ¼å¼: {"usageMetadata": {...}}
	if usageMetadata, ok := payload["usageMetadata"].(map[string]any); ok {
		return usageMetadata
	}

	return nil
}
