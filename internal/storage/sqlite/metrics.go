package sqlite

import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"time"

	"ccLoad/internal/model"
)

func (s *SQLiteStore) Aggregate(ctx context.Context, since time.Time, bucket time.Duration) ([]model.MetricPoint, error) {
	// æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨SQL GROUP BYè¿›è¡Œæ•°æ®åº“å±‚èšåˆï¼Œé¿å…å†…å­˜èšåˆ
	// åŸæ–¹æ¡ˆï¼šåŠ è½½æ‰€æœ‰æ—¥å¿—åˆ°å†…å­˜èšåˆï¼ˆ10ä¸‡æ¡æ—¥å¿—éœ€2-5ç§’ï¼Œå ç”¨100-200MBå†…å­˜ï¼‰
	// æ–°æ–¹æ¡ˆï¼šæ•°æ®åº“èšåˆï¼ˆæŸ¥è¯¢æ—¶é—´-80%ï¼Œå†…å­˜å ç”¨-90%ï¼‰
	// æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°æ¶ˆé™¤N+1é—®é¢˜ï¼ˆ100æ¸ é“åœºæ™¯æå‡50-100å€ï¼‰

	bucketSeconds := int64(bucket.Seconds())
	sinceUnix := since.Unix()

	// SQLèšåˆæŸ¥è¯¢ï¼šä½¿ç”¨Unixæ—¶é—´æˆ³é™¤æ³•å®ç°æ—¶é—´æ¡¶åˆ†ç»„ï¼ˆä» logDBï¼‰
	// æ€§èƒ½ä¼˜åŒ–ï¼štimeå­—æ®µä¸ºBIGINTæ¯«ç§’æ—¶é—´æˆ³ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡10-100å€
	// bucket_ts = (unix_timestamp_seconds / bucket_seconds) * bucket_seconds
	query := `
		SELECT
			((time / 1000) / ?) * ? AS bucket_ts,
			channel_id,
			SUM(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) AS success,
			SUM(CASE WHEN status_code < 200 OR status_code >= 300 THEN 1 ELSE 0 END) AS error,
			ROUND(
				AVG(CASE WHEN is_streaming = 1 AND first_byte_time > 0 AND status_code >= 200 AND status_code < 300 THEN first_byte_time ELSE NULL END),
				3
			) as avg_first_byte_time,
			ROUND(
				AVG(CASE WHEN duration > 0 AND status_code >= 200 AND status_code < 300 THEN duration ELSE NULL END),
				3
			) as avg_duration,
			SUM(CASE WHEN is_streaming = 1 AND first_byte_time > 0 AND status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) as stream_success_first_byte_count,
			SUM(CASE WHEN duration > 0 AND status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) as duration_success_count,
			SUM(COALESCE(cost, 0.0)) as total_cost
		FROM logs
		WHERE (time / 1000) >= ?
		GROUP BY bucket_ts, channel_id
		ORDER BY bucket_ts ASC
	`

	rows, err := s.logDB.QueryContext(ctx, query, bucketSeconds, bucketSeconds, sinceUnix)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	// è§£æèšåˆç»“æœï¼ŒæŒ‰æ—¶é—´æ¡¶é‡ç»„
	mapp := make(map[int64]*model.MetricPoint)
	channelIDsToFetch := make(map[int64]bool)
	// ç”¨äºè®¡ç®—æ€»ä½“å¹³å‡å€¼çš„è¾…åŠ©ç»“æ„
	type aggregationHelper struct {
		totalFirstByteTime float64 // æ€»é¦–å­—å“åº”æ—¶é—´
		firstByteCount     int     // æœ‰é¦–å­—å“åº”æ—¶é—´çš„è¯·æ±‚æ•°
		totalDuration      float64 // æ€»è€—æ—¶
		durationCount      int     // æœ‰æ€»è€—æ—¶çš„è¯·æ±‚æ•°
	}
	helperMap := make(map[int64]*aggregationHelper)

	for rows.Next() {
		var bucketTs int64
		var channelID sql.NullInt64
		var success, errorCount int
		var avgFirstByteTime sql.NullFloat64
		var avgDuration sql.NullFloat64
		var streamSuccessFirstByteCount int
		var durationSuccessCount int
		var totalCost float64

		if err := rows.Scan(&bucketTs, &channelID, &success, &errorCount, &avgFirstByteTime, &avgDuration, &streamSuccessFirstByteCount, &durationSuccessCount, &totalCost); err != nil {
			return nil, err
		}

		// è·å–æˆ–åˆ›å»ºæ—¶é—´æ¡¶
		mp, ok := mapp[bucketTs]
		if !ok {
			mp = &model.MetricPoint{
				Ts:       time.Unix(bucketTs, 0),
				Channels: make(map[string]model.ChannelMetric),
			}
			mapp[bucketTs] = mp
		}

		// è·å–æˆ–åˆ›å»ºè¾…åŠ©ç»“æ„
		helper, ok := helperMap[bucketTs]
		if !ok {
			helper = &aggregationHelper{}
			helperMap[bucketTs] = helper
		}

		// æ›´æ–°æ€»ä½“ç»Ÿè®¡
		mp.Success += success
		mp.Error += errorCount

		// ç´¯åŠ æ€»è´¹ç”¨
		if mp.TotalCost == nil {
			mp.TotalCost = new(float64)
		}
		*mp.TotalCost += totalCost

		// ç´¯åŠ é¦–å­—å“åº”æ—¶é—´æ•°æ®ï¼ˆç”¨äºåç»­è®¡ç®—å¹³å‡å€¼ï¼‰
		if avgFirstByteTime.Valid {
			helper.totalFirstByteTime += avgFirstByteTime.Float64 * float64(streamSuccessFirstByteCount)
			helper.firstByteCount += streamSuccessFirstByteCount
		}

		// ç´¯åŠ æ€»è€—æ—¶æ•°æ®ï¼ˆç”¨äºåç»­è®¡ç®—å¹³å‡å€¼ï¼‰
		if avgDuration.Valid {
			helper.totalDuration += avgDuration.Float64 * float64(durationSuccessCount)
			helper.durationCount += durationSuccessCount
		}

		// æš‚æ—¶ä½¿ç”¨ channel_id ä½œä¸º keyï¼Œç¨åæ›¿æ¢ä¸º name
		channelKey := "æœªçŸ¥æ¸ é“"
		if channelID.Valid {
			channelKey = fmt.Sprintf("ch_%d", channelID.Int64)
			channelIDsToFetch[channelID.Int64] = true
		}

		// å‡†å¤‡æ¸ é“æŒ‡æ ‡çš„æŒ‡é’ˆ
		var avgFBT *float64
		if avgFirstByteTime.Valid {
			avgFBT = new(float64)
			*avgFBT = avgFirstByteTime.Float64
		}
		var avgDur *float64
		if avgDuration.Valid {
			avgDur = new(float64)
			*avgDur = avgDuration.Float64
		}
		var chCost *float64
		if totalCost > 0 {
			chCost = new(float64)
			*chCost = totalCost
		}

		mp.Channels[channelKey] = model.ChannelMetric{
			Success:                 success,
			Error:                   errorCount,
			AvgFirstByteTimeSeconds: avgFBT,
			AvgDurationSeconds:      avgDur,
			TotalCost:               chCost,
		}
	}

	// æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°
	channelNames := make(map[int64]string)
	if len(channelIDsToFetch) > 0 {
		var err error
		channelNames, err = s.fetchChannelNamesBatch(ctx, channelIDsToFetch)
		if err != nil {
			// é™çº§å¤„ç†ï¼šæŸ¥è¯¢å¤±è´¥ä¸å½±å“èšåˆè¿”å›ï¼Œä»…è®°å½•é”™è¯¯
			log.Printf("âš ï¸  æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°å¤±è´¥: %v", err)
			channelNames = make(map[int64]string)
		}
	}

	// æ›¿æ¢ channel_id ä¸º channel_name å¹¶è®¡ç®—æ€»ä½“å¹³å‡é¦–å­—å“åº”æ—¶é—´
	for bucketTs, mp := range mapp {
		newChannels := make(map[string]model.ChannelMetric)
		for key, metric := range mp.Channels {
			if key == "æœªçŸ¥æ¸ é“" {
				newChannels[key] = metric
			} else {
				// è§£æ ch_123 æ ¼å¼
				var channelID int64
				fmt.Sscanf(key, "ch_%d", &channelID)
				if name, ok := channelNames[channelID]; ok {
					newChannels[name] = metric
				} else {
					newChannels["æœªçŸ¥æ¸ é“"] = metric
				}
			}
		}
		mp.Channels = newChannels

		// è®¡ç®—æ€»ä½“å¹³å‡é¦–å­—å“åº”æ—¶é—´
		if helper, ok := helperMap[bucketTs]; ok && helper.firstByteCount > 0 {
			avgFBT := helper.totalFirstByteTime / float64(helper.firstByteCount)
			mp.AvgFirstByteTimeSeconds = new(float64)
			*mp.AvgFirstByteTimeSeconds = avgFBT
			mp.FirstByteSampleCount = helper.firstByteCount
		}

		// è®¡ç®—æ€»ä½“å¹³å‡æ€»è€—æ—¶
		if helper, ok := helperMap[bucketTs]; ok && helper.durationCount > 0 {
			avgDur := helper.totalDuration / float64(helper.durationCount)
			mp.AvgDurationSeconds = new(float64)
			*mp.AvgDurationSeconds = avgDur
			mp.DurationSampleCount = helper.durationCount
		}
	}

	// ç”Ÿæˆå®Œæ•´çš„æ—¶é—´åºåˆ—ï¼ˆå¡«å……ç©ºæ¡¶ï¼‰
	out := []model.MetricPoint{}
	now := time.Now()
	endTime := now.Truncate(bucket).Add(bucket)
	startTime := since.Truncate(bucket)

	for t := startTime; t.Before(endTime); t = t.Add(bucket) {
		ts := t.Unix()
		if mp, ok := mapp[ts]; ok {
			out = append(out, *mp)
		} else {
			out = append(out, model.MetricPoint{
				Ts:       t,
				Channels: make(map[string]model.ChannelMetric),
			})
		}
	}

	// å·²æŒ‰æ—¶é—´å‡åºï¼ˆGROUP BY bucket_ts ASCï¼‰
	return out, nil
}

// AggregateRange èšåˆæŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æŒ‡æ ‡æ•°æ®ï¼ˆæ”¯æŒç²¾ç¡®æ—¥æœŸèŒƒå›´å¦‚"æ˜¨æ—¥"ï¼‰
func (s *SQLiteStore) AggregateRange(ctx context.Context, since, until time.Time, bucket time.Duration) ([]model.MetricPoint, error) {
	bucketSeconds := int64(bucket.Seconds())
	sinceUnix := since.Unix()
	untilUnix := until.Unix()

	query := `
		SELECT
			((time / 1000) / ?) * ? AS bucket_ts,
			channel_id,
			SUM(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) AS success,
			SUM(CASE WHEN status_code < 200 OR status_code >= 300 THEN 1 ELSE 0 END) AS error,
			ROUND(
				AVG(CASE WHEN is_streaming = 1 AND first_byte_time > 0 AND status_code >= 200 AND status_code < 300 THEN first_byte_time ELSE NULL END),
				3
			) as avg_first_byte_time,
			ROUND(
				AVG(CASE WHEN duration > 0 AND status_code >= 200 AND status_code < 300 THEN duration ELSE NULL END),
				3
			) as avg_duration,
			SUM(CASE WHEN is_streaming = 1 AND first_byte_time > 0 AND status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) as stream_success_first_byte_count,
			SUM(CASE WHEN duration > 0 AND status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) as duration_success_count,
			SUM(COALESCE(cost, 0.0)) as total_cost
		FROM logs
		WHERE (time / 1000) >= ? AND (time / 1000) <= ?
		GROUP BY bucket_ts, channel_id
		ORDER BY bucket_ts ASC
	`

	rows, err := s.logDB.QueryContext(ctx, query, bucketSeconds, bucketSeconds, sinceUnix, untilUnix)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	mapp := make(map[int64]*model.MetricPoint)
	channelIDsToFetch := make(map[int64]bool)
	type aggregationHelper struct {
		totalFirstByteTime float64
		firstByteCount     int
		totalDuration      float64
		durationCount      int
	}
	helperMap := make(map[int64]*aggregationHelper)

	for rows.Next() {
		var bucketTs int64
		var channelID sql.NullInt64
		var success, errorCount int
		var avgFirstByteTime sql.NullFloat64
		var avgDuration sql.NullFloat64
		var streamSuccessFirstByteCount int
		var durationSuccessCount int
		var totalCost float64

		if err := rows.Scan(&bucketTs, &channelID, &success, &errorCount, &avgFirstByteTime, &avgDuration, &streamSuccessFirstByteCount, &durationSuccessCount, &totalCost); err != nil {
			return nil, err
		}

		mp, ok := mapp[bucketTs]
		if !ok {
			mp = &model.MetricPoint{
				Ts:       time.Unix(bucketTs, 0),
				Channels: make(map[string]model.ChannelMetric),
			}
			mapp[bucketTs] = mp
		}

		helper, ok := helperMap[bucketTs]
		if !ok {
			helper = &aggregationHelper{}
			helperMap[bucketTs] = helper
		}

		mp.Success += success
		mp.Error += errorCount

		if mp.TotalCost == nil {
			mp.TotalCost = new(float64)
		}
		*mp.TotalCost += totalCost

		if avgFirstByteTime.Valid {
			helper.totalFirstByteTime += avgFirstByteTime.Float64 * float64(streamSuccessFirstByteCount)
			helper.firstByteCount += streamSuccessFirstByteCount
		}

		if avgDuration.Valid {
			helper.totalDuration += avgDuration.Float64 * float64(durationSuccessCount)
			helper.durationCount += durationSuccessCount
		}

		channelKey := "æœªçŸ¥æ¸ é“"
		if channelID.Valid {
			channelKey = fmt.Sprintf("ch_%d", channelID.Int64)
			channelIDsToFetch[channelID.Int64] = true
		}

		var avgFBT *float64
		if avgFirstByteTime.Valid {
			avgFBT = new(float64)
			*avgFBT = avgFirstByteTime.Float64
		}
		var avgDur *float64
		if avgDuration.Valid {
			avgDur = new(float64)
			*avgDur = avgDuration.Float64
		}
		var chCost *float64
		if totalCost > 0 {
			chCost = new(float64)
			*chCost = totalCost
		}

		mp.Channels[channelKey] = model.ChannelMetric{
			Success:                 success,
			Error:                   errorCount,
			AvgFirstByteTimeSeconds: avgFBT,
			AvgDurationSeconds:      avgDur,
			TotalCost:               chCost,
		}
	}

	channelNames := make(map[int64]string)
	if len(channelIDsToFetch) > 0 {
		var err error
		channelNames, err = s.fetchChannelNamesBatch(ctx, channelIDsToFetch)
		if err != nil {
			log.Printf("âš ï¸  æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°å¤±è´¥: %v", err)
			channelNames = make(map[int64]string)
		}
	}

	for bucketTs, mp := range mapp {
		newChannels := make(map[string]model.ChannelMetric)
		for key, metric := range mp.Channels {
			if key == "æœªçŸ¥æ¸ é“" {
				newChannels[key] = metric
			} else {
				var channelID int64
				fmt.Sscanf(key, "ch_%d", &channelID)
				if name, ok := channelNames[channelID]; ok {
					newChannels[name] = metric
				} else {
					newChannels["æœªçŸ¥æ¸ é“"] = metric
				}
			}
		}
		mp.Channels = newChannels

		if helper, ok := helperMap[bucketTs]; ok && helper.firstByteCount > 0 {
			avgFBT := helper.totalFirstByteTime / float64(helper.firstByteCount)
			mp.AvgFirstByteTimeSeconds = new(float64)
			*mp.AvgFirstByteTimeSeconds = avgFBT
			mp.FirstByteSampleCount = helper.firstByteCount
		}

		if helper, ok := helperMap[bucketTs]; ok && helper.durationCount > 0 {
			avgDur := helper.totalDuration / float64(helper.durationCount)
			mp.AvgDurationSeconds = new(float64)
			*mp.AvgDurationSeconds = avgDur
			mp.DurationSampleCount = helper.durationCount
		}
	}

	out := []model.MetricPoint{}
	endTime := until.Truncate(bucket).Add(bucket)
	startTime := since.Truncate(bucket)

	for t := startTime; t.Before(endTime); t = t.Add(bucket) {
		ts := t.Unix()
		if mp, ok := mapp[ts]; ok {
			out = append(out, *mp)
		} else {
			out = append(out, model.MetricPoint{
				Ts:       t,
				Channels: make(map[string]model.ChannelMetric),
			})
		}
	}

	return out, nil
}

// GetStats å®ç°ç»Ÿè®¡åŠŸèƒ½ï¼ŒæŒ‰æ¸ é“å’Œæ¨¡å‹ç»Ÿè®¡æˆåŠŸ/å¤±è´¥æ¬¡æ•°ï¼ˆä» logDBï¼‰
// æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°æ¶ˆé™¤N+1é—®é¢˜ï¼ˆ100æ¸ é“åœºæ™¯æå‡50-100å€ï¼‰
func (s *SQLiteStore) GetStats(ctx context.Context, startTime, endTime time.Time, filter *model.LogFilter) ([]model.StatsEntry, error) {
	// ä½¿ç”¨æŸ¥è¯¢æ„å»ºå™¨æ„å»ºç»Ÿè®¡æŸ¥è¯¢(ä» logDB)
	baseQuery := `
		SELECT
			channel_id,
			COALESCE(model, '') AS model,
			SUM(CASE WHEN status_code >= 200 AND status_code < 300 THEN 1 ELSE 0 END) AS success,
			SUM(CASE WHEN status_code < 200 OR status_code >= 300 THEN 1 ELSE 0 END) AS error,
			COUNT(*) AS total,
			ROUND(
				AVG(CASE WHEN is_streaming = 1 AND first_byte_time > 0 AND status_code >= 200 AND status_code < 300 THEN first_byte_time ELSE NULL END),
				3
			) as avg_first_byte_time,
			SUM(COALESCE(input_tokens, 0)) as total_input_tokens,
			SUM(COALESCE(output_tokens, 0)) as total_output_tokens,
			SUM(COALESCE(cache_read_input_tokens, 0)) as total_cache_read_input_tokens,
			SUM(COALESCE(cache_creation_input_tokens, 0)) as total_cache_creation_input_tokens,
			SUM(COALESCE(cost, 0.0)) as total_cost
		FROM logs`

	// timeå­—æ®µç°åœ¨æ˜¯BIGINTæ¯«ç§’æ—¶é—´æˆ³
	startMs := startTime.UnixMilli()
	endMs := endTime.UnixMilli()

	qb := NewQueryBuilder(baseQuery).
		Where("time >= ?", startMs).
		Where("time <= ?", endMs).
		Where("channel_id > 0") // ğŸ¯ æ ¸å¿ƒä¿®æ”¹:æ’é™¤channel_id=0çš„æ— æ•ˆè®°å½•

	// åº”ç”¨æ¸ é“ç±»å‹æˆ–åç§°è¿‡æ»¤
	_, isEmpty, err := s.applyChannelFilter(ctx, qb, filter)
	if err != nil {
		return nil, err
	}
	if isEmpty {
		return []model.StatsEntry{}, nil
	}

	// åº”ç”¨å…¶ä½™è¿‡æ»¤å™¨ï¼ˆæ¨¡å‹/çŠ¶æ€ç ç­‰ï¼‰
	qb.ApplyFilter(filter)

	suffix := "GROUP BY channel_id, model ORDER BY channel_id ASC, model ASC"
	query, args := qb.BuildWithSuffix(suffix)

	rows, err := s.logDB.QueryContext(ctx, query, args...)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var stats []model.StatsEntry
	channelIDsToFetch := make(map[int64]bool)

	for rows.Next() {
		var entry model.StatsEntry
		var avgFirstByteTime sql.NullFloat64
		var totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens sql.NullInt64
		var totalCost sql.NullFloat64

		err := rows.Scan(&entry.ChannelID, &entry.Model,
			&entry.Success, &entry.Error, &entry.Total, &avgFirstByteTime,
			&totalInputTokens, &totalOutputTokens, &totalCacheReadTokens, &totalCacheCreationTokens, &totalCost)
		if err != nil {
			return nil, err
		}

		if avgFirstByteTime.Valid {
			entry.AvgFirstByteTimeSeconds = &avgFirstByteTime.Float64
		}

		// å¡«å……tokenç»Ÿè®¡å­—æ®µï¼ˆä»…å½“æœ‰å€¼æ—¶ï¼‰
		if totalInputTokens.Valid && totalInputTokens.Int64 > 0 {
			entry.TotalInputTokens = &totalInputTokens.Int64
		}
		if totalOutputTokens.Valid && totalOutputTokens.Int64 > 0 {
			entry.TotalOutputTokens = &totalOutputTokens.Int64
		}
		if totalCacheReadTokens.Valid && totalCacheReadTokens.Int64 > 0 {
			entry.TotalCacheReadInputTokens = &totalCacheReadTokens.Int64
		}
		if totalCacheCreationTokens.Valid && totalCacheCreationTokens.Int64 > 0 {
			entry.TotalCacheCreationInputTokens = &totalCacheCreationTokens.Int64
		}
		if totalCost.Valid && totalCost.Float64 > 0 {
			entry.TotalCost = &totalCost.Float64
		}

		if entry.ChannelID != nil {
			channelIDsToFetch[int64(*entry.ChannelID)] = true
		}
		stats = append(stats, entry)
	}

	if len(channelIDsToFetch) > 0 {
		channelNames, err := s.fetchChannelNamesBatch(ctx, channelIDsToFetch)
		if err != nil {
			// é™çº§å¤„ç†:æŸ¥è¯¢å¤±è´¥ä¸å½±å“ç»Ÿè®¡è¿”å›,ä»…è®°å½•é”™è¯¯
			log.Printf("âš ï¸  æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°å¤±è´¥: %v", err)
			channelNames = make(map[int64]string)
		}

		// å¡«å……æ¸ é“åç§°
		for i := range stats {
			if stats[i].ChannelID != nil {
				if name, ok := channelNames[int64(*stats[i].ChannelID)]; ok {
					stats[i].ChannelName = name
				} else {
					// å¦‚æœæŸ¥è¯¢ä¸åˆ°æ¸ é“åç§°,ä½¿ç”¨"æœªçŸ¥æ¸ é“"æ ‡è¯†
					stats[i].ChannelName = "æœªçŸ¥æ¸ é“"
				}
			}
		}
	}

	return stats, nil
}
