package mysql

import (
	"ccLoad/internal/model"
	"context"
	"database/sql"
	"fmt"
	"log"
	"strings"
	"time"
)

// AggregateRangeWithFilter èšåˆæŒ‡å®šæ—¶é—´èŒƒå›´ã€æ¸ é“ç±»åž‹å’Œæ¨¡åž‹çš„æŒ‡æ ‡æ•°æ®
// channelType ä¸ºç©ºå­—ç¬¦ä¸²æ—¶è¿”å›žæ‰€æœ‰æ¸ é“ç±»åž‹çš„æ•°æ®
// modelFilter ä¸ºç©ºå­—ç¬¦ä¸²æ—¶è¿”å›žæ‰€æœ‰æ¨¡åž‹çš„æ•°æ®
func (s *MySQLStore) AggregateRangeWithFilter(ctx context.Context, since, until time.Time, bucket time.Duration, channelType string, modelFilter string) ([]model.MetricPoint, error) {
	bucketSeconds := int64(bucket.Seconds())
	sinceUnix := since.Unix()
	untilUnix := until.Unix()

	// ðŸŽ¯ ä¿®å¤è·¨æ•°æ®åº“JOINï¼šå…ˆä»Žä¸»åº“æŸ¥è¯¢ç¬¦åˆç±»åž‹çš„æ¸ é“IDåˆ—è¡¨
	var channelIDs []int64
	if channelType != "" {
		var err error
		channelIDs, err = s.fetchChannelIDsByType(ctx, channelType)
		if err != nil {
			return nil, fmt.Errorf("fetch channel ids by type: %w", err)
		}
		// å¦‚æžœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¸ é“ï¼Œç›´æŽ¥è¿”å›žç©ºç»“æžœ
		if len(channelIDs) == 0 {
			return buildEmptyMetricPoints(since, until, bucket), nil
		}
	}

	// æž„å»ºæŸ¥è¯¢ï¼šä¸å†JOIN channelsè¡¨ï¼Œä½¿ç”¨INå­å¥è¿‡æ»¤
	query := `
		SELECT
			((logs.time / 1000) / ?) * ? AS bucket_ts,
			logs.channel_id,
			SUM(CASE WHEN logs.status_code >= 200 AND logs.status_code < 300 THEN 1 ELSE 0 END) AS success,
			SUM(CASE WHEN logs.status_code < 200 OR logs.status_code >= 300 THEN 1 ELSE 0 END) AS error,
			ROUND(
				AVG(CASE WHEN logs.is_streaming = 1 AND logs.first_byte_time > 0 AND logs.status_code >= 200 AND logs.status_code < 300 THEN logs.first_byte_time ELSE NULL END),
				3
			) as avg_first_byte_time,
			ROUND(
				AVG(CASE WHEN logs.duration > 0 AND logs.status_code >= 200 AND logs.status_code < 300 THEN logs.duration ELSE NULL END),
				3
			) as avg_duration,
			SUM(CASE WHEN logs.is_streaming = 1 AND logs.first_byte_time > 0 AND logs.status_code >= 200 AND logs.status_code < 300 THEN 1 ELSE 0 END) as stream_success_first_byte_count,
			SUM(CASE WHEN logs.duration > 0 AND logs.status_code >= 200 AND logs.status_code < 300 THEN 1 ELSE 0 END) as duration_success_count,
			SUM(COALESCE(logs.cost, 0.0)) as total_cost
		FROM logs
		WHERE (logs.time / 1000) >= ? AND (logs.time / 1000) <= ?
	`

	args := []any{bucketSeconds, bucketSeconds, sinceUnix, untilUnix}

	// æ·»åŠ  channel_type è¿‡æ»¤ï¼ˆä½¿ç”¨INå­å¥ï¼‰
	if len(channelIDs) > 0 {
		placeholders := make([]string, len(channelIDs))
		for i := range channelIDs {
			placeholders[i] = "?"
			args = append(args, channelIDs[i])
		}
		query += fmt.Sprintf(" AND logs.channel_id IN (%s)", strings.Join(placeholders, ","))
	}

	// æ·»åŠ æ¨¡åž‹è¿‡æ»¤
	if modelFilter != "" {
		query += " AND logs.model = ?"
		args = append(args, modelFilter)
	}

	query += `
		GROUP BY bucket_ts, logs.channel_id
		ORDER BY bucket_ts ASC
	`

	rows, err := s.db.QueryContext(ctx, query, args...)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	mapp := make(map[int64]*model.MetricPoint)
	channelIDsToFetch := make(map[int64]bool)
	type aggregationHelper struct {
		totalFirstByteTime float64
		firstByteCount     int
		totalDuration      float64
		durationCount      int
	}
	helperMap := make(map[int64]*aggregationHelper)

	for rows.Next() {
		var bucketTs int64
		var channelID sql.NullInt64
		var success, errorCount int
		var avgFirstByteTime sql.NullFloat64
		var avgDuration sql.NullFloat64
		var streamSuccessFirstByteCount int
		var durationSuccessCount int
		var totalCost float64

		if err := rows.Scan(&bucketTs, &channelID, &success, &errorCount, &avgFirstByteTime, &avgDuration, &streamSuccessFirstByteCount, &durationSuccessCount, &totalCost); err != nil {
			return nil, err
		}

		mp, ok := mapp[bucketTs]
		if !ok {
			mp = &model.MetricPoint{
				Ts:       time.Unix(bucketTs, 0),
				Channels: make(map[string]model.ChannelMetric),
			}
			mapp[bucketTs] = mp
		}

		helper, ok := helperMap[bucketTs]
		if !ok {
			helper = &aggregationHelper{}
			helperMap[bucketTs] = helper
		}

		mp.Success += success
		mp.Error += errorCount

		if mp.TotalCost == nil {
			mp.TotalCost = new(float64)
		}
		*mp.TotalCost += totalCost

		if avgFirstByteTime.Valid {
			helper.totalFirstByteTime += avgFirstByteTime.Float64 * float64(streamSuccessFirstByteCount)
			helper.firstByteCount += streamSuccessFirstByteCount
		}

		if avgDuration.Valid {
			helper.totalDuration += avgDuration.Float64 * float64(durationSuccessCount)
			helper.durationCount += durationSuccessCount
		}

		channelKey := "æœªçŸ¥æ¸ é“"
		if channelID.Valid {
			channelKey = fmt.Sprintf("ch_%d", channelID.Int64)
			channelIDsToFetch[channelID.Int64] = true
		}

		var avgFBT *float64
		if avgFirstByteTime.Valid {
			avgFBT = new(float64)
			*avgFBT = avgFirstByteTime.Float64
		}
		var avgDur *float64
		if avgDuration.Valid {
			avgDur = new(float64)
			*avgDur = avgDuration.Float64
		}
		var chCost *float64
		if totalCost > 0 {
			chCost = new(float64)
			*chCost = totalCost
		}

		mp.Channels[channelKey] = model.ChannelMetric{
			Success:                 success,
			Error:                   errorCount,
			AvgFirstByteTimeSeconds: avgFBT,
			AvgDurationSeconds:      avgDur,
			TotalCost:               chCost,
		}
	}

	channelNames := make(map[int64]string)
	if len(channelIDsToFetch) > 0 {
		var err error
		channelNames, err = s.fetchChannelNamesBatch(ctx, channelIDsToFetch)
		if err != nil {
			log.Printf("âš ï¸  æ‰¹é‡æŸ¥è¯¢æ¸ é“åç§°å¤±è´¥: %v", err)
			channelNames = make(map[int64]string)
		}
	}

	for bucketTs, mp := range mapp {
		newChannels := make(map[string]model.ChannelMetric)
		for key, metric := range mp.Channels {
			if key == "æœªçŸ¥æ¸ é“" {
				newChannels[key] = metric
			} else {
				var channelID int64
				fmt.Sscanf(key, "ch_%d", &channelID)
				if name, ok := channelNames[channelID]; ok {
					newChannels[name] = metric
				} else {
					newChannels["æœªçŸ¥æ¸ é“"] = metric
				}
			}
		}
		mp.Channels = newChannels

		if helper, ok := helperMap[bucketTs]; ok && helper.firstByteCount > 0 {
			avgFBT := helper.totalFirstByteTime / float64(helper.firstByteCount)
			mp.AvgFirstByteTimeSeconds = new(float64)
			*mp.AvgFirstByteTimeSeconds = avgFBT
			mp.FirstByteSampleCount = helper.firstByteCount
		}

		if helper, ok := helperMap[bucketTs]; ok && helper.durationCount > 0 {
			avgDur := helper.totalDuration / float64(helper.durationCount)
			mp.AvgDurationSeconds = new(float64)
			*mp.AvgDurationSeconds = avgDur
			mp.DurationSampleCount = helper.durationCount
		}
	}

	out := []model.MetricPoint{}
	endTime := until.Truncate(bucket).Add(bucket)
	startTime := since.Truncate(bucket)

	for t := startTime; t.Before(endTime); t = t.Add(bucket) {
		ts := t.Unix()
		if mp, ok := mapp[ts]; ok {
			out = append(out, *mp)
		} else {
			out = append(out, model.MetricPoint{
				Ts:       t,
				Channels: make(map[string]model.ChannelMetric),
			})
		}
	}

	return out, nil
}

// buildEmptyMetricPoints æž„å»ºç©ºçš„æ—¶é—´åºåˆ—æ•°æ®ç‚¹ï¼ˆç”¨äºŽæ— æ•°æ®åœºæ™¯ï¼‰
func buildEmptyMetricPoints(since, until time.Time, bucket time.Duration) []model.MetricPoint {
	var out []model.MetricPoint
	endTime := until.Truncate(bucket).Add(bucket)
	startTime := since.Truncate(bucket)

	for t := startTime; t.Before(endTime); t = t.Add(bucket) {
		out = append(out, model.MetricPoint{
			Ts:       t,
			Channels: make(map[string]model.ChannelMetric),
		})
	}
	return out
}

// GetDistinctModels èŽ·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„åŽ»é‡æ¨¡åž‹åˆ—è¡¨
func (s *MySQLStore) GetDistinctModels(ctx context.Context, since, until time.Time) ([]string, error) {
	query := `
		SELECT DISTINCT model
		FROM logs
		WHERE (time / 1000) >= ? AND (time / 1000) <= ? AND model != ''
		ORDER BY model
	`
	
	rows, err := s.db.QueryContext(ctx, query, since.Unix(), until.Unix())
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var models []string
	for rows.Next() {
		var model string
		if err := rows.Scan(&model); err != nil {
			continue
		}
		models = append(models, model)
	}

	return models, nil
}
